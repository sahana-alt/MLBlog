[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BlogsML1",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nUnderstanding Probability Theory and Random Variables\n\n\n\n\n\n\n\nrandom variable\n\n\nprobability\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nSwapnil Singh\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Clustering\n\n\n\n\n\n\n\nclustering\n\n\nunsupervised learning\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nSwapnil Singh\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression in Python\n\n\n\n\n\n\n\nregression\n\n\nsupervised learning\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nSwapnil Singh\n\n\n\n\n\n\n  \n\n\n\n\nClassification: A way via ensembling\n\n\n\n\n\n\n\nclassification\n\n\nensemble\n\n\nsupervised learning\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2023\n\n\nSwapnil Singh\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection using DBScan\n\n\n\n\n\n\n\nanomaly detection\n\n\nclustering\n\n\nunsupervised learning\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nSwapnil Singh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html",
    "href": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html",
    "title": "Understanding Probability Theory and Random Variables",
    "section": "",
    "text": "Probability Theory and Random Variables are fundamental concepts in statistics and machine learning that underpin various analytical and predictive techniques. Understanding these concepts is crucial for building a strong foundation in data science and applied mathematics. In this blog, we will delve into the basics of Probability Theory and Random Variables, accompanied by Python code examples and visualizations. Additionally, we will demonstrate how these concepts are utilized in practical machine learning applications."
  },
  {
    "objectID": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html#what-is-probability-theory",
    "href": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html#what-is-probability-theory",
    "title": "Understanding Probability Theory and Random Variables",
    "section": "What is Probability Theory?",
    "text": "What is Probability Theory?\nProbability Theory is a branch of mathematics that deals with the analysis of random phenomena. It provides a framework for quantifying uncertainty and making predictions based on available information. The core of Probability Theory revolves around the concept of events, probabilities, and their relationships. In the context of machine learning, probability theory forms the basis for various algorithms, including Bayesian methods and probabilistic models."
  },
  {
    "objectID": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html#code-example-simulating-coin-flips",
    "href": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html#code-example-simulating-coin-flips",
    "title": "Understanding Probability Theory and Random Variables",
    "section": "Code Example: Simulating Coin Flips",
    "text": "Code Example: Simulating Coin Flips\nFigure 1 visualises the probability for head and tails based on the dummy data created\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\"\"\"Simulating coin flips\"\"\"\nresults = np.random.choice(['Heads', 'Tails'], size=1000)\n\n\"\"\"Calculating probabilities\"\"\"\nhead_count = np.sum(results == 'Heads')\ntail_count = np.sum(results == 'Tails')\nhead_probability = head_count / 1000\ntail_probability = tail_count / 1000\n\n\"\"\"Visualizing the results\"\"\"\nlabels = ['Heads', 'Tails']\nprobabilities = [head_probability, tail_probability]\nplt.bar(labels, probabilities)\nplt.title('Coin Flip Simulation')\nplt.show()\n\n\n\n\n\nFigure 1: Coin Flip Simulation"
  },
  {
    "objectID": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html#what-are-random-variables",
    "href": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html#what-are-random-variables",
    "title": "Understanding Probability Theory and Random Variables",
    "section": "What are Random Variables?",
    "text": "What are Random Variables?\nRandom Variables are variables that can take on various values in a random experiment. They represent the outcomes of random processes and can be discrete or continuous. Random Variables play a crucial role in defining the probability distribution of events and formulating statistical models for data analysis."
  },
  {
    "objectID": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html#code-example-generating-normal-distribution-from-random-data",
    "href": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html#code-example-generating-normal-distribution-from-random-data",
    "title": "Understanding Probability Theory and Random Variables",
    "section": "Code Example: Generating Normal Distribution from Random Data",
    "text": "Code Example: Generating Normal Distribution from Random Data\nFigure 2 visualises the normal distribution on 1000 random values generated for mean 0 and standard diviation of 0.1\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters for the normal distribution\nmu, sigma = 0, 0.1 # mean and standard deviation\n\n# Generating data points for the normal distribution\ns = np.random.normal(mu, sigma, 1000)\n\n# Plotting the histogram\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * \n         np.exp( - (bins - mu)**2 / (2 * sigma**2) ), \n         linewidth=2, color='r')\nplt.title('Normal Distribution')\nplt.show()\n\n\n\n\n\nFigure 2: Normal Distribution"
  },
  {
    "objectID": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html#machine-learning-and-probability-theory",
    "href": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html#machine-learning-and-probability-theory",
    "title": "Understanding Probability Theory and Random Variables",
    "section": "Machine Learning and Probability Theory",
    "text": "Machine Learning and Probability Theory\nProbability Theory serves as the backbone of various machine learning algorithms, enabling the modeling of uncertainties and making informed predictions. It is extensively used in areas such as classification, regression, and clustering. Algorithms like Naive Bayes, Gaussian Processes, and Hidden Markov Models heavily rely on the principles of Probability Theory. Below given is an implementation of Gaussian Naive Bayes classification using"
  },
  {
    "objectID": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html#code-example-gaussian-naive-bayes-classifier",
    "href": "posts/Understanding-Probability-Theory-and-Random-Variables/index.html#code-example-gaussian-naive-bayes-classifier",
    "title": "Understanding Probability Theory and Random Variables",
    "section": "Code Example: Gaussian Naive Bayes Classifier",
    "text": "Code Example: Gaussian Naive Bayes Classifier\nFigure 3 visualizes the results of Gaussian Naive Bayes model on the Iris Dataset. Figure 3 (a) shows the confusion matrix, Figure 3 (b) shows the RoC Curve, and Figure 3 (c) shows the Precision Recall Cruve of Gaussian Naive Bayes model on the Iris Dataset\n\n\n\nCode\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn import metrics\nfrom itertools import cycle\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Loading the iris dataset\niris = pd.read_csv('iris_data.csv')\nX = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\ny = iris['species']\ny = label_binarize(y, classes=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# Training the Gaussian Naive Bayes model\nclf = GaussianNB()\nclf.fit(X_train, np.argmax(y_train, axis=1))\n\n# Making predictions and evaluating the model\ny_test_pred = clf.predict(X_test)\ny_test_proba = clf.predict_proba(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(np.argmax(y_test,axis=1), y_test_pred))\nprint(\"Precision:\", metrics.precision_score(np.argmax(y_test,axis=1), y_test_pred, average='weighted'))\nprint(\"Recall:\", metrics.recall_score(np.argmax(y_test,axis=1), y_test_pred, average='weighted'))\nprint(\"sensitivity:\", metrics.recall_score(np.argmax(y_test,axis=1), y_test_pred, average='weighted'))\nprint(\"f1 score:\", metrics.f1_score(np.argmax(y_test,axis=1), y_test_pred, average='weighted'))\nprint(metrics.classification_report(np.argmax(y_test,axis=1), y_test_pred, target_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']))\ncm = metrics.confusion_matrix(np.argmax(y_test,axis=1), y_test_pred)\nplt.figure(figsize=(4, 3), dpi=600)\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nclasses = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes)\nplt.yticks(tick_marks, classes)\nthresh = cm.max() / 2.\nfor i, j in np.ndindex(cm.shape):\n    plt.text(j, i, cm[i, j],\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] &gt; thresh else \"black\")\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(len(classes)):\n    fpr[i], tpr[i], _ = metrics.roc_curve(y_test[:, i], y_test_proba[:, i])\n    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(y_test.ravel(), y_test_proba.ravel())\nroc_auc[\"micro\"] = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Plot the ROC curve\nplt.figure()\nlw = 2\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(len(classes)), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(classes[i], roc_auc[i]))\n\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='Micro-average ROC curve (area = {0:0.2f})'\n         ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(len(classes)):\n    precision[i], recall[i], _ = metrics.precision_recall_curve(y_test[:, i], y_test_proba[:, i])\n    average_precision[i] = metrics.average_precision_score(y_test[:, i], y_test_proba[:, i])\n\n# Compute micro-average precision-recall curve\nprecision[\"micro\"], recall[\"micro\"], _ = metrics.precision_recall_curve(y_test.ravel(), y_test_proba.ravel())\naverage_precision[\"micro\"] = metrics.average_precision_score(y_test, y_test_proba, average=\"micro\")\n\n# Plot Precision-Recall curve for each class\nplt.figure()\nplt.step(recall['micro'], precision['micro'], where='post', label='Micro-average curve (area = {0:0.2f})'\n         ''.format(average_precision[\"micro\"]))\nfor i in range(len(classes)):\n    plt.step(recall[i], precision[i], where='post', label='Class {0} (area = {1:0.2f})'\n             ''.format(classes[i], average_precision[i]))\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"best\")\nplt.show()\n\n\nAccuracy: 0.9333333333333333\nPrecision: 0.9352007469654529\nRecall: 0.9333333333333333\nsensitivity: 0.9333333333333333\nf1 score: 0.933615520282187\n                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        14\nIris-versicolor       0.94      0.89      0.91        18\n Iris-virginica       0.86      0.92      0.89        13\n\n       accuracy                           0.93        45\n      macro avg       0.93      0.94      0.93        45\n   weighted avg       0.94      0.93      0.93        45\n\n\n\n\n\n\n(a) Confusion Matrix\n\n\n\n\n\n\n\n(b) RoC Curve\n\n\n\n\n\n\n\n(c) Precision Recall Curve\n\n\n\nFigure 3: Classification Results"
  },
  {
    "objectID": "posts/Classification-A-way-via-ensembling/index.html",
    "href": "posts/Classification-A-way-via-ensembling/index.html",
    "title": "Classification: A way via ensembling",
    "section": "",
    "text": "Classification is a fundamental task in machine learning that involves categorizing input data into predefined classes. Ensemble methods are powerful techniques that combine multiple individual models to enhance predictive performance and robustness. This blog provides an overview of popular ensemble methods for classification and demonstrates their implementation in Python, including the visualization of important evaluation metrics such as the ROC curve, PR curve, and confusion matrix."
  },
  {
    "objectID": "posts/Classification-A-way-via-ensembling/index.html#python-implimentation",
    "href": "posts/Classification-A-way-via-ensembling/index.html#python-implimentation",
    "title": "Classification: A way via ensembling",
    "section": "Python implimentation",
    "text": "Python implimentation\nFigure 1 visualizes the results of Stacking Classifier on the Breast Cancer. Figure 1 (a) shows the confusion matrix, Figure 1 (b) shows the RoC Curve, and Figure 1 (c) shows the Precision Recall Cruve of Stacking Classifier on the Breast Cancer\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport numpy as  np\nfrom itertools import cycle\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load the Breast Cancer dataset\ndata = pd.read_csv('breast_cancer.csv')\nX = data.iloc[:,2:-1]\ny = data[['diagnosis']]\ny = label_binarize(y, classes=['M','B'])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n\n# Initialize the base classifiers\nestimators = [\n    ('lr', LogisticRegression(max_iter=500)),\n    ('dt', DecisionTreeClassifier()),\n    ('rf', RandomForestClassifier()),\n    ('knn', KNeighborsClassifier())\n]\n\n# Initialize the stacking classifier\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=500))\n\n# Fit the model on the training data\nclf.fit(X_train, y_train)\n\n# Evaluate the model on the test data\ny_test_pred = clf.predict(X_test)\ny_test_proba = clf.predict_proba(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_test_pred))\nprint(\"Precision:\", metrics.precision_score(y_test, y_test_pred, average='weighted'))\nprint(\"Recall:\", metrics.recall_score(y_test, y_test_pred, average='weighted'))\nprint(\"sensitivity:\", metrics.recall_score(y_test, y_test_pred, average='weighted'))\nprint(\"f1 score:\", metrics.f1_score(y_test, y_test_pred, average='weighted'))\nprint(metrics.classification_report(y_test, y_test_pred, target_names = ['M','B']))\ncm = metrics.confusion_matrix(y_test, y_test_pred)\nplt.figure(figsize=(4, 3), dpi=600)\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nclasses = ['M','B']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes)\nplt.yticks(tick_marks, classes)\nthresh = cm.max() / 2.\nfor i, j in np.ndindex(cm.shape):\n    plt.text(j, i, cm[i, j],\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] &gt; thresh else \"black\")\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n# Compute ROC curve and ROC area\nfpr, tpr, _ = metrics.roc_curve(y_test, y_test_proba[:, 1])\nroc_auc = metrics.auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\nplt.plot([0, 1], [0, 1], 'k--', color='grey', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n# Compute precision-recall curve and area\nprecision, recall, _ = metrics.precision_recall_curve(y_test, y_test_proba[:, 1])\npr_auc = metrics.average_precision_score(y_test, y_test_proba[:, 1])\n\n# Plot Precision-Recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label=f'AUC = {pr_auc:.2f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"lower left\")\nplt.show()\n\n\nAccuracy: 0.9736842105263158\nPrecision: 0.97475106685633\nRecall: 0.9736842105263158\nsensitivity: 0.9736842105263158\nf1 score: 0.9734808562744625\n              precision    recall  f1-score   support\n\n           M       1.00      0.93      0.96        43\n           B       0.96      1.00      0.98        71\n\n    accuracy                           0.97       114\n   macro avg       0.98      0.97      0.97       114\nweighted avg       0.97      0.97      0.97       114\n\n\n\n\n\n\n(a) Confusion Matrix\n\n\n\n\n\n\n\n(b) RoC Curve\n\n\n\n\n\n\n\n(c) Precision Recall Curve\n\n\n\nFigure 1: Stacking Results"
  },
  {
    "objectID": "posts/Classification-A-way-via-ensembling/index.html#python-implimentation-1",
    "href": "posts/Classification-A-way-via-ensembling/index.html#python-implimentation-1",
    "title": "Classification: A way via ensembling",
    "section": "Python implimentation",
    "text": "Python implimentation\nFigure 1 visualizes the results of Bagging Classifier on the Breast Cancer. Figure 1 (a) shows the confusion matrix, Figure 2 (b) shows the RoC Curve, and Figure 1 (c) shows the Precision Recall Cruve of Bagging Classifier on the Breast Cancer\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport numpy as  np\nfrom itertools import cycle\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load the Breast Cancer dataset\ndata = pd.read_csv('breast_cancer.csv')\nX = data.iloc[:,2:-1]\ny = data[['diagnosis']]\ny = label_binarize(y, classes=['M','B'])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n\n# Initialize the base classifiers\ndt = DecisionTreeClassifier()\n\n# Initialize the stacking classifier\nclf = BaggingClassifier(estimator=dt, n_estimators=10)\n\n# Fit the model on the training data\nclf.fit(X_train, y_train)\n\n# Evaluate the model on the test data\ny_test_pred = clf.predict(X_test)\ny_test_proba = clf.predict_proba(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_test_pred))\nprint(\"Precision:\", metrics.precision_score(y_test, y_test_pred, average='weighted'))\nprint(\"Recall:\", metrics.recall_score(y_test, y_test_pred, average='weighted'))\nprint(\"sensitivity:\", metrics.recall_score(y_test, y_test_pred, average='weighted'))\nprint(\"f1 score:\", metrics.f1_score(y_test, y_test_pred, average='weighted'))\nprint(metrics.classification_report(y_test, y_test_pred, target_names = ['M','B']))\ncm = metrics.confusion_matrix(y_test, y_test_pred)\nplt.figure(figsize=(4, 3), dpi=600)\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nclasses = ['M','B']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes)\nplt.yticks(tick_marks, classes)\nthresh = cm.max() / 2.\nfor i, j in np.ndindex(cm.shape):\n    plt.text(j, i, cm[i, j],\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] &gt; thresh else \"black\")\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n# Compute ROC curve and ROC area\nfpr, tpr, _ = metrics.roc_curve(y_test, y_test_proba[:, 1])\nroc_auc = metrics.auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\nplt.plot([0, 1], [0, 1], 'k--', color='grey', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n# Compute precision-recall curve and area\nprecision, recall, _ = metrics.precision_recall_curve(y_test, y_test_proba[:, 1])\npr_auc = metrics.average_precision_score(y_test, y_test_proba[:, 1])\n\n# Plot Precision-Recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label=f'AUC = {pr_auc:.2f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"lower left\")\nplt.show()\n\n\nAccuracy: 0.956140350877193\nPrecision: 0.9560881370091896\nRecall: 0.956140350877193\nsensitivity: 0.956140350877193\nf1 score: 0.9560357083576897\n              precision    recall  f1-score   support\n\n           M       0.95      0.93      0.94        43\n           B       0.96      0.97      0.97        71\n\n    accuracy                           0.96       114\n   macro avg       0.96      0.95      0.95       114\nweighted avg       0.96      0.96      0.96       114\n\n\n\n\n\n\n(a) Confusion Matrix\n\n\n\n\n\n\n\n(b) RoC Curve\n\n\n\n\n\n\n\n(c) Precision Recall Curve\n\n\n\nFigure 2: Bagging Results"
  },
  {
    "objectID": "posts/Classification-A-way-via-ensembling/index.html#python-implimentation-2",
    "href": "posts/Classification-A-way-via-ensembling/index.html#python-implimentation-2",
    "title": "Classification: A way via ensembling",
    "section": "Python implimentation",
    "text": "Python implimentation\nFigure 3 visualizes the results of Boosting Classifier on the Breast Cancer. Figure 3 (a) shows the confusion matrix, Figure 3 (b) shows the RoC Curve, and Figure 3 (c) shows the Precision Recall Cruve of Boosting Classifier on the Breast Cancer\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport numpy as  np\nfrom itertools import cycle\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load the Breast Cancer dataset\nX = data.iloc[:,2:-1]\ny = data[['diagnosis']]\ny = label_binarize(y, classes=['M','B'])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n\n# Initialize the base classifiers\ndt = DecisionTreeClassifier()\n\n# Initialize the stacking classifier\nclf = AdaBoostClassifier(estimator=dt, n_estimators=10)\n\n# Fit the model on the training data\nclf.fit(X_train, y_train)\n\n# Evaluate the model on the test data\ny_test_pred = clf.predict(X_test)\ny_test_proba = clf.predict_proba(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_test_pred))\nprint(\"Precision:\", metrics.precision_score(y_test, y_test_pred, average='weighted'))\nprint(\"Recall:\", metrics.recall_score(y_test, y_test_pred, average='weighted'))\nprint(\"sensitivity:\", metrics.recall_score(y_test, y_test_pred, average='weighted'))\nprint(\"f1 score:\", metrics.f1_score(y_test, y_test_pred, average='weighted'))\nprint(metrics.classification_report(y_test, y_test_pred, target_names = ['M', 'B']))\ncm = metrics.confusion_matrix(y_test, y_test_pred)\nplt.figure(figsize=(4, 3), dpi=600)\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nclasses = ['M', 'B']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes)\nplt.yticks(tick_marks, classes)\nthresh = cm.max() / 2.\nfor i, j in np.ndindex(cm.shape):\n    plt.text(j, i, cm[i, j],\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] &gt; thresh else \"black\")\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n# Compute ROC curve and ROC area\nfpr, tpr, _ = metrics.roc_curve(y_test, y_test_proba[:, 1])\nroc_auc = metrics.auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\nplt.plot([0, 1], [0, 1], 'k--', color='grey', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n# Compute precision-recall curve and area\nprecision, recall, _ = metrics.precision_recall_curve(y_test, y_test_proba[:, 1])\npr_auc = metrics.average_precision_score(y_test, y_test_proba[:, 1])\n\n# Plot Precision-Recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label=f'AUC = {pr_auc:.2f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"lower left\")\nplt.show()\n\n\nAccuracy: 0.9473684210526315\nPrecision: 0.9473684210526315\nRecall: 0.9473684210526315\nsensitivity: 0.9473684210526315\nf1 score: 0.9473684210526315\n              precision    recall  f1-score   support\n\n           M       0.93      0.93      0.93        43\n           B       0.96      0.96      0.96        71\n\n    accuracy                           0.95       114\n   macro avg       0.94      0.94      0.94       114\nweighted avg       0.95      0.95      0.95       114\n\n\n\n\n\n\n(a) Confusion Matrix\n\n\n\n\n\n\n\n(b) RoC Curve\n\n\n\n\n\n\n\n(c) Precision Recall Curve\n\n\n\nFigure 3: Boosting Results"
  },
  {
    "objectID": "posts/Anomaly-Detection-using-DBScan/index.html",
    "href": "posts/Anomaly-Detection-using-DBScan/index.html",
    "title": "Anomaly Detection using DBScan",
    "section": "",
    "text": "Anomaly detection is a crucial task in data analysis, aimed at identifying rare items, events, or observations that differ significantly from the majority of the data. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a powerful clustering algorithm known for its ability to identify clusters of any shape within a dataset. Leveraging its ability to capture the local density of data points, DBSCAN can be a useful tool for anomaly detection. In this blog, we will explore the concept of anomaly detection using the DBSCAN algorithm, including an in-depth explanation, its application in detecting outliers, and a step-by-step implementation in Python."
  },
  {
    "objectID": "posts/Anomaly-Detection-using-DBScan/index.html#python-implimentation",
    "href": "posts/Anomaly-Detection-using-DBScan/index.html#python-implimentation",
    "title": "Anomaly Detection using DBScan",
    "section": "Python implimentation",
    "text": "Python implimentation\nFigure 1 shows the clusters for anomaly detection. Figure 1 (a) shows the dummy data and Figure 1 (b) shows the dummy data in clusters after dbscan but does not have any anomalies. Figure 1 (c) shows anomalies in the dummy data with some noise using dbscan. Figure 1 (d) shows anomalies in the Iris dataset using dbscan.\n\n\n\nCode\n# importing libraries\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs, make_moons, load_iris\nfrom numpy import random, where\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# setting random seed for getting the same results\nrandom.seed(7)\n# making clusters with standard diviation of 0.3\nx,y=make_blobs(n_samples=200,centers=1,cluster_std=0.3)\n# plotting the data points\nplt.scatter(x[:,0],x[:,1])\nplt.show()\n\n# creating the dbscan model with minmum samples near the centroid as 20 and cluster radius as 0.28\n# # decreasing min samples will decrease outliers and decreasing eps will increase outliers\nmodel=DBSCAN(eps=0.28,min_samples=20)\n# predicting the cluster lables\npred=model.fit_predict(x)\n\n# getting the index of outliers and printing them\noutlier_index = where(pred==-1)\n\n# getting the value of outliers\noutlier_value=x[outlier_index]\n\n# creating a dataset with noise so that there can be outliers\n# the clusters are well defined if the noise is lower and outliers are also low \nx,y=make_moons(n_samples=400,noise=0.4,random_state=1)\n# plotting the dataset\nplt.scatter(x[:,0],x[:,1],c=y)\nplt.show()\n\n# creating the dbscan model with minmum samples near the centroid as 5 and cluster radiusas 0.2\nmodel=DBSCAN(eps=0.2,min_samples=5)\n# assigning clusters\npred=model.fit_predict(x)\n\n# get index of outliers and print them\noutlier_index=where(pred==-1)\n\n# get value of outliers and print them\noutlier_value=x[outlier_index]\n\n# plot scatter plot of the dataset\nplt.scatter(x[:,0],x[:,1],c=y)\n\n# plotting outliers in red\nplt.scatter(outlier_value[:,0],outlier_value[:,1],color='r')\nplt.show()\n\ndata = load_iris()\n# creating the dbscan model with minmum samples near the centroid as 5 and cluster radiusas 0.5\nmodel=DBSCAN(eps=0.5,min_samples=5)\n# fitting the data\nmodel.fit(data.data)\n\npca=PCA(n_components=2).fit(data.data)\npca_scatter=pca.transform(data.data)\n\n# storing the shape of the pca\nsh=pca_scatter.shape\n\n# plotting data and outliers\nfor i in range(0,sh[0]):\n    if model.labels_[i]==0:\n        cluster1=plt.scatter(pca_scatter[i,0],pca_scatter[i,1],c='y',marker='x')\n    elif model.labels_[i]==1:\n        cluster2=plt.scatter(pca_scatter[i,0],pca_scatter[i,1],c='g',marker='+')\n    else:\n        cluster3=plt.scatter(pca_scatter[i,0],pca_scatter[i,1],c='r',marker='o')\nplt.legend([cluster1,cluster2,cluster3],['label0','label1','outlier'])\nplt.show()\n\n\n\n\n\n(a) Plot of dummy data\n\n\n\n\n\n\n\n(b) Plot after DBScan\n\n\n\n\n\n\n\n(c) Plot after DBScan on dummy data with noise showing anomalies\n\n\n\n\n\n\n\n(d) Anomaly Detection on Iris Dataset\n\n\n\nFigure 1: Anomaly Detection Results"
  },
  {
    "objectID": "posts/Linear-and-Nonlinear-Regression-in-Python/index.html",
    "href": "posts/Linear-and-Nonlinear-Regression-in-Python/index.html",
    "title": "Linear and Nonlinear Regression in Python",
    "section": "",
    "text": "Regression analysis is a powerful statistical method used to understand the relationship between a dependent variable and one or more independent variables. It is widely employed in various fields to make predictions and infer insights from data. In this blog, we’ll delve into both linear and nonlinear regression and demonstrate how to implement them in Python."
  },
  {
    "objectID": "posts/Linear-and-Nonlinear-Regression-in-Python/index.html#python-implementation",
    "href": "posts/Linear-and-Nonlinear-Regression-in-Python/index.html#python-implementation",
    "title": "Linear and Nonlinear Regression in Python",
    "section": "Python Implementation",
    "text": "Python Implementation\n\nRandomly Generated Sampels\nFigure 1 visualizes the linear regression curve on 5 dummy points\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generating linear data\nX = np.array([1, 2, 3, 4, 5]).reshape((-1, 1))\ny = np.array([2, 3.9, 6.1, 8.2, 9.8])\n\n# Fitting linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\ny_pred = model.predict(X)\n\n# Visualizing linear regression\nplt.scatter(X, y, color='blue', label='Data Points')\nplt.plot(X, y_pred, color='red', label='Linear Regression Line')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\nplt.show()\n\nslope = model.coef_[0]\nintercept = model.intercept_\n\n# Print the regression equation\nprint(f\"Regression Equation: y = {slope} * X + {intercept}\")\n\n\n\n\n\nFigure 1: Linear Regression on Dummy Data\n\n\n\n\nRegression Equation: y = 1.9900000000000007 * X + 0.029999999999997584\n\n\n\n\nCOVID-19 Severity Score Dataset\n\n\nCode\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load the covid-19 severity dataset\ndata = pd.read_csv('COVID_19_CT_Severity_Score.csv')\nX = data.iloc[:,1:-2]\ny = data.iloc[:,-2]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Create and fit the model\nmodel = LinearRegression()\nmodel.fit(X_train.values, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test.values)\n\nprint('R2 Score: ', metrics.r2_score(y_test, y_pred))\nprint('MSE: ', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE: ', (metrics.mean_squared_error(y_test, y_pred))**2)\n\ncoefficients = model.coef_\nintercept = model.intercept_\n\n# Print the regression equation\nequation = \"Regression Equation: y = {:.3f}\".format(intercept)\ncols = list(X.columns)\nfor i, coef in enumerate(coefficients):\n    equation += (\" + {:.3f} * \"+cols[i]).format(coef, i)\nprint(equation)\n\n\nR2 Score:  0.32169205167088033\nMSE:  17.241074996660384\nRMSE:  297.25466704046784\nRegression Equation: y = 2.474 + 0.037 * Age + 0.790 * Gender + 4.576 * GGO + 1.953 * Consolidation + 5.238 * Crazy_paving"
  },
  {
    "objectID": "posts/Linear-and-Nonlinear-Regression-in-Python/index.html#python-implementation-1",
    "href": "posts/Linear-and-Nonlinear-Regression-in-Python/index.html#python-implementation-1",
    "title": "Linear and Nonlinear Regression in Python",
    "section": "Python Implementation",
    "text": "Python Implementation\n\nNon Linear Curve Fitting\nFigure 2 visualizes the non linear regression curve on 5 dummy points\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\n# Generating nonlinear data\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([2.5, 3.9, 6.3, 9.2, 14.1])\n\n# Defining a nonlinear function\ndef nonlinear_func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\n# Fitting nonlinear regression model\nparams, covariance = curve_fit(nonlinear_func, X, y)\na, b, c = params\ny_pred = nonlinear_func(X, a, b, c)\n\n# Visualizing nonlinear regression\nplt.scatter(X, y, color='blue', label='Data Points')\nplt.plot(X, y_pred, color='red', label='Nonlinear Regression Curve')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.legend()\nplt.show()\n\n\n\n\n\nFigure 2: Non Linear Regression on Dummy Data\n\n\n\n\n\n\nRandom Forest Regression\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load the diabetes dataset\ndata = pd.read_csv('COVID_19_CT_Severity_Score.csv')\nX = data.iloc[:,1:-2]\ny = data.iloc[:,-2]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Fitting the Random Forest Regression model\nregressor = RandomForestRegressor(n_estimators=100, random_state=0)\nregressor.fit(X_train.values, y_train)\n\n# Predicting the values\ny_pred = regressor.predict(X_test.values)\n\nprint('R2 Score: ', metrics.r2_score(y_test, y_pred))\nprint('MSE: ', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE: ', (metrics.mean_squared_error(y_test, y_pred))**2)\n\n\nR2 Score:  0.23162348318152914\nMSE:  19.53041709856552\nRMSE:  381.43719204394034"
  },
  {
    "objectID": "posts/Understanding-Clustering/index.html",
    "href": "posts/Understanding-Clustering/index.html",
    "title": "Understanding Clustering",
    "section": "",
    "text": "Clustering is a fundamental technique in unsupervised learning, used to group similar data points together. In this blog, we will explore three popular clustering algorithms: KMeans, DBSCAN, and Agglomerative Clustering. We’ll provide a brief overview of each algorithm, discuss their applications, and provide Python code examples with visualizations to demonstrate how they work."
  },
  {
    "objectID": "posts/Understanding-Clustering/index.html#kmeans-clustering",
    "href": "posts/Understanding-Clustering/index.html#kmeans-clustering",
    "title": "Understanding Clustering",
    "section": "KMeans Clustering",
    "text": "KMeans Clustering\nKMeans is an iterative algorithm that partitions data into K clusters. It works by assigning data points to the nearest cluster center and updating the center as the mean of the assigned points. This process continues until convergence."
  },
  {
    "objectID": "posts/Understanding-Clustering/index.html#dbscan-clustering",
    "href": "posts/Understanding-Clustering/index.html#dbscan-clustering",
    "title": "Understanding Clustering",
    "section": "DBSCAN Clustering",
    "text": "DBSCAN Clustering\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm. It groups together points that are closely packed and marks points that lie alone in low-density regions as outliers."
  },
  {
    "objectID": "posts/Understanding-Clustering/index.html#agglomerative-clustering",
    "href": "posts/Understanding-Clustering/index.html#agglomerative-clustering",
    "title": "Understanding Clustering",
    "section": "Agglomerative Clustering",
    "text": "Agglomerative Clustering\nAgglomerative Clustering is a hierarchical clustering technique that starts with each point as a separate cluster and merges the closest clusters iteratively until only one cluster remains."
  },
  {
    "objectID": "posts/Understanding-Clustering/index.html#kmeans-clustering-1",
    "href": "posts/Understanding-Clustering/index.html#kmeans-clustering-1",
    "title": "Understanding Clustering",
    "section": "KMeans Clustering",
    "text": "KMeans Clustering\nFigure 1 visualises the clusters created after kmeans clustering. Figure 1 (a) displays the correlation heat map to identify the most correlated features. It is observed that Annual Income (k$) and Spending Socre (1-100) are the most correlated features. Figure 1 (b) shows the elbow curve indicating that 4 clusters would be better to use and Figure 1 (c) shows the clusters after clustering.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport warnings\nimport pandas as pd\nwarnings.filterwarnings(\"ignore\")\n\ndf = pd.read_csv('Mall_Customers.csv')\nX = df[['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n\n\n\ncorr_matrix = np.corrcoef(X.values, rowvar=False)\nplt.imshow(corr_matrix, cmap='Wistia', interpolation='nearest')\nfor i in range(corr_matrix.shape[0]):\n    for j in range(corr_matrix.shape[1]):\n        plt.text(j, i, f'{corr_matrix[i, j]:.2f}', ha='center', va='center', color='black')\nplt.title('Heatmap for the Data', fontsize=20)\nplt.xticks(np.arange(corr_matrix.shape[0]), labels=['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)'], rotation=90)\nplt.yticks(np.arange(corr_matrix.shape[1]), labels=['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)'])\nplt.colorbar()\nplt.show()\n\nX = X.values\ndist = [] \nfor i in range(1,11): \n  km = KMeans(n_clusters=i, random_state=42).fit(X)\n  dist.append(km.inertia_)\nplt.plot(range(1,11), dist, marker='*')\nplt.xlabel('Number of cluster')\nplt.ylabel('Distortion')\nplt.show()\n\n# Applying KMeans algorithm\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\ncenters = kmeans.cluster_centers_\n\n# Visualizing clusters with different colors and a legend\nplt.scatter(X[y_kmeans == 0, 2], X[y_kmeans == 0, 3], c='yellow', s=50, cmap='viridis', label='Cluster1')\nplt.scatter(X[y_kmeans == 1, 2], X[y_kmeans == 1, 3], c='blue', s=50, cmap='viridis', label='Cluster2')\nplt.scatter(X[y_kmeans == 2, 2], X[y_kmeans == 2, 3], c='green', s=50, cmap='viridis', label='Cluster3')\nplt.scatter(X[y_kmeans == 3, 2], X[y_kmeans == 3, 3], c='violet', s=50, cmap='viridis', label='Cluster4')\nplt.scatter(centers[:, 2], centers[:, 3], c='red', s=200, alpha=0.5, label='Centroids')\nplt.legend()\nplt.show()\n\nprint('Silhouette Score: ',metrics.silhouette_score(X, km.labels_, metric='euclidean'))\n\n\n\n\n\n(a) Correlation Heat Map to identify correlated features\n\n\n\n\n\n\n\n(b) Elbow Curve\n\n\n\n\n\n\n\n(c) Cluster Visualization\n\n\n\n\nSilhouette Score:  0.3724780422340438\n\nFigure 1: KMeans Clustering"
  },
  {
    "objectID": "posts/Understanding-Clustering/index.html#dbscan-clustering-1",
    "href": "posts/Understanding-Clustering/index.html#dbscan-clustering-1",
    "title": "Understanding Clustering",
    "section": "DBSCAN Clustering",
    "text": "DBSCAN Clustering\nFigure 2 visualises the clusters created after DBScan\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import DBSCAN\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Generate sample data\nX, _ = make_moons(n_samples=1000, noise=0.05)\n\n# Apply DBSCAN algorithm\ndbscan = DBSCAN(eps=0.1, min_samples=5)\ny_pred = dbscan.fit_predict(X)\n\n# Visualize Clusters\nplt.scatter(X[y_pred == 0, 0], X[y_pred == 0, 1], c='lightblue', marker='o', edgecolor='black', label='Cluster 1')\nplt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], c='lightgreen', marker='s', edgecolor='black', label='Cluster 2')\nplt.legend()\nplt.show()\n\n\n\n\n\nFigure 2: DBSCAN Clusters Created"
  },
  {
    "objectID": "posts/Understanding-Clustering/index.html#agglomerative-clustering-1",
    "href": "posts/Understanding-Clustering/index.html#agglomerative-clustering-1",
    "title": "Understanding Clustering",
    "section": "Agglomerative Clustering",
    "text": "Agglomerative Clustering\nFigure 3 visualises the dendogram created after aglomerative clustering on the mall customer dataset\n\n\nCode\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.cluster import AgglomerativeClustering\nimport pandas as pd\n\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx &lt; n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack(\n        [model.children_, model.distances_, counts]\n    ).astype(float)\n\n    # Plot the corresponding dendrogram\n    dendrogram(linkage_matrix, **kwargs)\n\n\ndf = pd.read_csv('Mall_Customers.csv')\nX = df[['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)']].values\n\n# setting distance_threshold=0 ensures we compute the full tree.\nmodel = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n\nmodel = model.fit(X)\nplt.title(\"Hierarchical Clustering Dendrogram\")\n# plot the top three levels of the dendrogram\nplot_dendrogram(model, truncate_mode=\"level\", p=3)\nplt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\nplt.show()\n\n\n\n\n\nFigure 3: Agglomerative Clustering Dendogram"
  },
  {
    "objectID": "sample.html",
    "href": "sample.html",
    "title": "BlogsML1",
    "section": "",
    "text": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn import metrics\nfrom itertools import cycle\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Loading the iris dataset\niris = pd.read_csv('datasets/iris_data.csv')\niris.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\nX = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\ny = iris[['species']]\ny = label_binarize(y, classes=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])\n\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# Training the Gaussian Naive Bayes model\nclf = GaussianNB()\nclf.fit(X_train, np.argmax(y_train, axis=1))\n\n# Making predictions and evaluating the model\ny_test_pred = clf.predict(X_test)\ny_test_proba = clf.predict_proba(X_test)\n\n\nprint(\"Accuracy:\",metrics.accuracy_score(np.argmax(y_test,axis=1), y_test_pred))\nprint(\"Precision:\", metrics.precision_score(np.argmax(y_test,axis=1), y_test_pred, average='weighted'))\nprint(\"Recall:\", metrics.recall_score(np.argmax(y_test,axis=1), y_test_pred, average='weighted'))\nprint(\"sensitivity:\", metrics.recall_score(np.argmax(y_test,axis=1), y_test_pred, average='weighted'))\nprint(\"f1 score:\", metrics.f1_score(np.argmax(y_test,axis=1), y_test_pred, average='weighted'))\nprint(metrics.classification_report(np.argmax(y_test,axis=1), y_test_pred, target_names = ['Setosa', 'Versicolor', 'Virginica']))\n\nAccuracy: 0.9333333333333333\nPrecision: 0.9352007469654529\nRecall: 0.9333333333333333\nsensitivity: 0.9333333333333333\nf1 score: 0.933615520282187\n              precision    recall  f1-score   support\n\n      Setosa       1.00      1.00      1.00        14\n  Versicolor       0.94      0.89      0.91        18\n   Virginica       0.86      0.92      0.89        13\n\n    accuracy                           0.93        45\n   macro avg       0.93      0.94      0.93        45\nweighted avg       0.94      0.93      0.93        45\n\n\n\n\ncm = metrics.confusion_matrix(np.argmax(y_test,axis=1), y_test_pred)\nplt.figure(figsize=(4, 3), dpi=600)\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nclasses = ['Setosa', 'Versicolor', 'Virginica']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes)\nplt.yticks(tick_marks, classes)\nthresh = cm.max() / 2.\nfor i, j in np.ndindex(cm.shape):\n    plt.text(j, i, cm[i, j],\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] &gt; thresh else \"black\")\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(len(classes)):\n    fpr[i], tpr[i], _ = metrics.roc_curve(y_test[:, i], y_test_proba[:, i])\n    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(y_test.ravel(), y_test_proba.ravel())\nroc_auc[\"micro\"] = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Plot the ROC curve\nplt.figure()\nlw = 2\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(len(classes)), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(classes[i], roc_auc[i]))\n\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='Micro-average ROC curve (area = {0:0.2f})'\n         ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(len(classes)):\n    precision[i], recall[i], _ = metrics.precision_recall_curve(y_test[:, i], y_test_proba[:, i])\n    average_precision[i] = metrics.average_precision_score(y_test[:, i], y_test_proba[:, i])\n\n# Compute micro-average precision-recall curve\nprecision[\"micro\"], recall[\"micro\"], _ = metrics.precision_recall_curve(y_test.ravel(), y_test_proba.ravel())\naverage_precision[\"micro\"] = metrics.average_precision_score(y_test, y_test_proba, average=\"micro\")\n\n# Plot Precision-Recall curve for each class\nplt.figure()\nplt.step(recall['micro'], precision['micro'], where='post', label='Micro-average curve (area = {0:0.2f})'\n         ''.format(average_precision[\"micro\"]))\nfor i in range(len(classes)):\n    plt.step(recall[i], precision[i], where='post', label='Class {0} (area = {1:0.2f})'\n             ''.format(classes[i], average_precision[i]))\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"best\")\nplt.show()"
  },
  {
    "objectID": "posts/Understanding-Clustering/index.html#dataset-exploration",
    "href": "posts/Understanding-Clustering/index.html#dataset-exploration",
    "title": "Understanding Clustering",
    "section": "Dataset Exploration",
    "text": "Dataset Exploration\nFigure 1 displays the correlation heat map to identify the most correlated features. It is observed that Annual Income (k$) and Spending Socre (1-100) are the most correlated features.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport warnings\nimport pandas as pd\nwarnings.filterwarnings(\"ignore\")\n\ndf = pd.read_csv('Mall_Customers.csv')\nX = df[['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\nsns.heatmap(X.corr(), cmap = 'Wistia', annot = True)\nplt.title('Heatmap for the Data', fontsize = 20)\n\n\n\nText(0.5, 1.0, 'Heatmap for the Data')\n(a) Correlation Heat Map to identify correlated features\n\n\n\n\n\n\n(b)\n\n\n\nFigure 1: ?(caption)"
  }
]