{
  "hash": "0fd4d6bdd6043a5502a61a939eea8ff2",
  "result": {
    "markdown": "---\ntitle: Understanding Probability Theory and Random Variables\nimage: image.jpeg\nauthor: Swapnil Singh\ndate: '2023-11-04'\ncategories:\n  - random variable\n  - probability\n  - statistics\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n---\n\nProbability Theory and Random Variables are fundamental concepts in statistics and machine learning that underpin various analytical and predictive techniques. Understanding these concepts is crucial for building a strong foundation in data science and applied mathematics. In this blog, we will delve into the basics of Probability Theory and Random Variables, accompanied by Python code examples and visualizations. Additionally, we will demonstrate how these concepts are utilized in practical machine learning applications.\n\n# Understanding Probability Theory\n\n## What is Probability Theory?\n\nProbability Theory is a branch of mathematics that deals with the analysis of random phenomena. It provides a framework for quantifying uncertainty and making predictions based on available information. The core of Probability Theory revolves around the concept of events, probabilities, and their relationships. In the context of machine learning, probability theory forms the basis for various algorithms, including Bayesian methods and probabilistic models.\n\n## Code Example: Simulating Coin Flips\n\n@fig-pro-sim visualises the probability for head and tails based on the dummy data created\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\"\"\"Simulating coin flips\"\"\"\nresults = np.random.choice(['Heads', 'Tails'], size=1000)\n\n\"\"\"Calculating probabilities\"\"\"\nhead_count = np.sum(results == 'Heads')\ntail_count = np.sum(results == 'Tails')\nhead_probability = head_count / 1000\ntail_probability = tail_count / 1000\n\n\"\"\"Visualizing the results\"\"\"\nlabels = ['Heads', 'Tails']\nprobabilities = [head_probability, tail_probability]\nplt.bar(labels, probabilities)\nplt.title('Coin Flip Simulation')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Coin Flip Simulation](index_files/figure-html/fig-pro-sim-output-1.png){#fig-pro-sim width=571 height=431}\n:::\n:::\n\n\n# Exploring Random Variables\n\n## What are Random Variables?\n\nRandom Variables are variables that can take on various values in a random experiment. They represent the outcomes of random processes and can be discrete or continuous. Random Variables play a crucial role in defining the probability distribution of events and formulating statistical models for data analysis.\n\n## Code Example: Generating Normal Distribution from Random Data\n\n@fig-norm-dist visualises the normal distribution on 1000 random values generated for mean 0 and standard diviation of 0.1\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters for the normal distribution\nmu, sigma = 0, 0.1 # mean and standard deviation\n\n# Generating data points for the normal distribution\ns = np.random.normal(mu, sigma, 1000)\n\n# Plotting the histogram\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * \n         np.exp( - (bins - mu)**2 / (2 * sigma**2) ), \n         linewidth=2, color='r')\nplt.title('Normal Distribution')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Normal Distribution](index_files/figure-html/fig-norm-dist-output-1.png){#fig-norm-dist width=558 height=431}\n:::\n:::\n\n\n# Application in Machine Learning\n\n## Machine Learning and Probability Theory\n\nProbability Theory serves as the backbone of various machine learning algorithms, enabling the modeling of uncertainties and making informed predictions. It is extensively used in areas such as classification, regression, and clustering. Algorithms like Naive Bayes, Gaussian Processes, and Hidden Markov Models heavily rely on the principles of Probability Theory. Below given is an implementation of Gaussian Naive Bayes classification using \n\n## Code Example: Gaussian Naive Bayes Classifier\n@fig-classification-results visualizes the results of Gaussian Naive Bayes model on the Iris Dataset. @fig-classification-results-1 shows the confusion matrix, @fig-classification-results-2 shows the RoC Curve, and  @fig-classification-results-3 shows the Precision Recall Cruve of Gaussian Naive Bayes model on the Iris Dataset\n\n::: {#fig-classification-results .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn import metrics\nfrom itertools import cycle\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Loading the iris dataset\niris = pd.read_csv('iris_data.csv')\nX = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\ny = iris['species']\ny = label_binarize(y, classes=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# Training the Gaussian Naive Bayes model\nclf = GaussianNB()\nclf.fit(X_train, np.argmax(y_train, axis=1))\n\n# Making predictions and evaluating the model\ny_test_pred = clf.predict(X_test)\ny_test_proba = clf.predict_proba(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(np.argmax(y_test,axis=1), y_test_pred))\nprint(\"Precision:\", metrics.precision_score(np.argmax(y_test,axis=1), y_test_pred, average='weighted'))\nprint(\"Recall:\", metrics.recall_score(np.argmax(y_test,axis=1), y_test_pred, average='weighted'))\nprint(\"sensitivity:\", metrics.recall_score(np.argmax(y_test,axis=1), y_test_pred, average='weighted'))\nprint(\"f1 score:\", metrics.f1_score(np.argmax(y_test,axis=1), y_test_pred, average='weighted'))\nprint(metrics.classification_report(np.argmax(y_test,axis=1), y_test_pred, target_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']))\ncm = metrics.confusion_matrix(np.argmax(y_test,axis=1), y_test_pred)\nplt.figure(figsize=(4, 3), dpi=600)\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nclasses = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes)\nplt.yticks(tick_marks, classes)\nthresh = cm.max() / 2.\nfor i, j in np.ndindex(cm.shape):\n    plt.text(j, i, cm[i, j],\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] > thresh else \"black\")\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(len(classes)):\n    fpr[i], tpr[i], _ = metrics.roc_curve(y_test[:, i], y_test_proba[:, i])\n    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(y_test.ravel(), y_test_proba.ravel())\nroc_auc[\"micro\"] = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Plot the ROC curve\nplt.figure()\nlw = 2\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(len(classes)), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(classes[i], roc_auc[i]))\n\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='Micro-average ROC curve (area = {0:0.2f})'\n         ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(len(classes)):\n    precision[i], recall[i], _ = metrics.precision_recall_curve(y_test[:, i], y_test_proba[:, i])\n    average_precision[i] = metrics.average_precision_score(y_test[:, i], y_test_proba[:, i])\n\n# Compute micro-average precision-recall curve\nprecision[\"micro\"], recall[\"micro\"], _ = metrics.precision_recall_curve(y_test.ravel(), y_test_proba.ravel())\naverage_precision[\"micro\"] = metrics.average_precision_score(y_test, y_test_proba, average=\"micro\")\n\n# Plot Precision-Recall curve for each class\nplt.figure()\nplt.step(recall['micro'], precision['micro'], where='post', label='Micro-average curve (area = {0:0.2f})'\n         ''.format(average_precision[\"micro\"]))\nfor i in range(len(classes)):\n    plt.step(recall[i], precision[i], where='post', label='Class {0} (area = {1:0.2f})'\n             ''.format(classes[i], average_precision[i]))\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"best\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.9333333333333333\nPrecision: 0.9352007469654529\nRecall: 0.9333333333333333\nsensitivity: 0.9333333333333333\nf1 score: 0.933615520282187\n                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        14\nIris-versicolor       0.94      0.89      0.91        18\n Iris-virginica       0.86      0.92      0.89        13\n\n       accuracy                           0.93        45\n      macro avg       0.93      0.94      0.93        45\n   weighted avg       0.94      0.93      0.93        45\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Confusion Matrix](index_files/figure-html/fig-classification-results-output-2.png){#fig-classification-results-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![RoC Curve](index_files/figure-html/fig-classification-results-output-3.png){#fig-classification-results-2 width=599 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![Precision Recall Curve](index_files/figure-html/fig-classification-results-output-4.png){#fig-classification-results-3 width=589 height=449}\n:::\n\nClassification Results\n:::\n\n\n# Conclusion\nIn conclusion, Probability Theory and Random Variables form the backbone of statistical analysis and machine learning. By grasping these concepts and their applications, data scientists can develop robust models and gain insights into complex data patterns. Utilizing Python libraries, such as NumPy, Matplotlib, and scikit-learn, allows for practical implementation and visualization of these concepts in real-world scenarios.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}