{
  "hash": "a61ded4125694ca01e8bda6a12d1577f",
  "result": {
    "markdown": "---\ntitle: 'Classification: A way via ensembling'\nimage: image.jpeg\nauthor: Swapnil Singh\ndate: '2023-11-07'\ncategories:\n  - classification\n  - ensemble\n  - supervised learning\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n---\n\nClassification is a fundamental task in machine learning that involves categorizing input data into predefined classes. Ensemble methods are powerful techniques that combine multiple individual models to enhance predictive performance and robustness. This blog provides an overview of popular ensemble methods for classification and demonstrates their implementation in Python, including the visualization of important evaluation metrics such as the ROC curve, PR curve, and confusion matrix.\n\n# Stacking\nStacking, also known as stacked generalization, is an ensemble learning technique that combines the predictions of multiple base models through a meta-learner to improve overall predictive performance. In stacking, the predictions from diverse base models are used as features to train a higher-level model, known as the meta-learner, which then generates the final predictions. This approach allows the meta-learner to learn how to weigh the outputs of the base models effectively, leveraging the strengths of each individual model to make more accurate and robust predictions.\n\n## Python implimentation\n@fig-stacking-results visualizes the results of Stacking Classifier on the Breast Cancer. @fig-stacking-results-1 shows the confusion matrix, @fig-stacking-results-2 shows the RoC Curve, and  @fig-stacking-results-3 shows the Precision Recall Cruve of Stacking Classifier on the Breast Cancer\n\n::: {#fig-stacking-results .cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport numpy as  np\nfrom itertools import cycle\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load the Breast Cancer dataset\ndata = pd.read_csv('breast_cancer.csv')\nX = data.iloc[:,2:-1]\ny = data[['diagnosis']]\ny = label_binarize(y, classes=['M','B'])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n\n# Initialize the base classifiers\nestimators = [\n    ('lr', LogisticRegression(max_iter=500)),\n    ('dt', DecisionTreeClassifier()),\n    ('rf', RandomForestClassifier()),\n    ('knn', KNeighborsClassifier())\n]\n\n# Initialize the stacking classifier\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=500))\n\n# Fit the model on the training data\nclf.fit(X_train, y_train)\n\n# Evaluate the model on the test data\ny_test_pred = clf.predict(X_test)\ny_test_proba = clf.predict_proba(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_test_pred))\nprint(\"Precision:\", metrics.precision_score(y_test, y_test_pred, average='weighted'))\nprint(\"Recall:\", metrics.recall_score(y_test, y_test_pred, average='weighted'))\nprint(\"sensitivity:\", metrics.recall_score(y_test, y_test_pred, average='weighted'))\nprint(\"f1 score:\", metrics.f1_score(y_test, y_test_pred, average='weighted'))\nprint(metrics.classification_report(y_test, y_test_pred, target_names = ['M','B']))\ncm = metrics.confusion_matrix(y_test, y_test_pred)\nplt.figure(figsize=(4, 3), dpi=600)\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nclasses = ['M','B']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes)\nplt.yticks(tick_marks, classes)\nthresh = cm.max() / 2.\nfor i, j in np.ndindex(cm.shape):\n    plt.text(j, i, cm[i, j],\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] > thresh else \"black\")\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n# Compute ROC curve and ROC area\nfpr, tpr, _ = metrics.roc_curve(y_test, y_test_proba[:, 1])\nroc_auc = metrics.auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\nplt.plot([0, 1], [0, 1], 'k--', color='grey', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n# Compute precision-recall curve and area\nprecision, recall, _ = metrics.precision_recall_curve(y_test, y_test_proba[:, 1])\npr_auc = metrics.average_precision_score(y_test, y_test_proba[:, 1])\n\n# Plot Precision-Recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label=f'AUC = {pr_auc:.2f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"lower left\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.9736842105263158\nPrecision: 0.97475106685633\nRecall: 0.9736842105263158\nsensitivity: 0.9736842105263158\nf1 score: 0.9734808562744625\n              precision    recall  f1-score   support\n\n           M       1.00      0.93      0.96        43\n           B       0.96      1.00      0.98        71\n\n    accuracy                           0.97       114\n   macro avg       0.98      0.97      0.97       114\nweighted avg       0.97      0.97      0.97       114\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Confusion Matrix](index_files/figure-html/fig-stacking-results-output-2.png){#fig-stacking-results-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![RoC Curve](index_files/figure-html/fig-stacking-results-output-3.png){#fig-stacking-results-2 width=663 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![Precision Recall Curve](index_files/figure-html/fig-stacking-results-output-4.png){#fig-stacking-results-3 width=672 height=523}\n:::\n\nStacking Results\n:::\n\n\n# Bagging\nBagging, or Bootstrap Aggregating, is a popular ensemble learning technique used to improve the stability and accuracy of machine learning models, particularly for high variance models. Bagging involves training multiple instances of the same learning algorithm on different subsets of the training data. Each subset is sampled with replacement, allowing for multiple subsets to contain duplicate instances, thus creating \"bootstrap\" samples.\n\n## Python implimentation\n@fig-stacking-results visualizes the results of Bagging Classifier on the Breast Cancer. @fig-stacking-results-1 shows the confusion matrix, @fig-bagging-results-2 shows the RoC Curve, and  @fig-stacking-results-3 shows the Precision Recall Cruve of Bagging Classifier on the Breast Cancer\n\n::: {#fig-bagging-results .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport numpy as  np\nfrom itertools import cycle\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load the Breast Cancer dataset\ndata = pd.read_csv('breast_cancer.csv')\nX = data.iloc[:,2:-1]\ny = data[['diagnosis']]\ny = label_binarize(y, classes=['M','B'])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n\n# Initialize the base classifiers\ndt = DecisionTreeClassifier()\n\n# Initialize the stacking classifier\nclf = BaggingClassifier(estimator=dt, n_estimators=10)\n\n# Fit the model on the training data\nclf.fit(X_train, y_train)\n\n# Evaluate the model on the test data\ny_test_pred = clf.predict(X_test)\ny_test_proba = clf.predict_proba(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_test_pred))\nprint(\"Precision:\", metrics.precision_score(y_test, y_test_pred, average='weighted'))\nprint(\"Recall:\", metrics.recall_score(y_test, y_test_pred, average='weighted'))\nprint(\"sensitivity:\", metrics.recall_score(y_test, y_test_pred, average='weighted'))\nprint(\"f1 score:\", metrics.f1_score(y_test, y_test_pred, average='weighted'))\nprint(metrics.classification_report(y_test, y_test_pred, target_names = ['M','B']))\ncm = metrics.confusion_matrix(y_test, y_test_pred)\nplt.figure(figsize=(4, 3), dpi=600)\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nclasses = ['M','B']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes)\nplt.yticks(tick_marks, classes)\nthresh = cm.max() / 2.\nfor i, j in np.ndindex(cm.shape):\n    plt.text(j, i, cm[i, j],\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] > thresh else \"black\")\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n# Compute ROC curve and ROC area\nfpr, tpr, _ = metrics.roc_curve(y_test, y_test_proba[:, 1])\nroc_auc = metrics.auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\nplt.plot([0, 1], [0, 1], 'k--', color='grey', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n# Compute precision-recall curve and area\nprecision, recall, _ = metrics.precision_recall_curve(y_test, y_test_proba[:, 1])\npr_auc = metrics.average_precision_score(y_test, y_test_proba[:, 1])\n\n# Plot Precision-Recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label=f'AUC = {pr_auc:.2f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"lower left\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.956140350877193\nPrecision: 0.9560881370091896\nRecall: 0.956140350877193\nsensitivity: 0.956140350877193\nf1 score: 0.9560357083576897\n              precision    recall  f1-score   support\n\n           M       0.95      0.93      0.94        43\n           B       0.96      0.97      0.97        71\n\n    accuracy                           0.96       114\n   macro avg       0.96      0.95      0.95       114\nweighted avg       0.96      0.96      0.96       114\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Confusion Matrix](index_files/figure-html/fig-bagging-results-output-2.png){#fig-bagging-results-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![RoC Curve](index_files/figure-html/fig-bagging-results-output-3.png){#fig-bagging-results-2 width=663 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![Precision Recall Curve](index_files/figure-html/fig-bagging-results-output-4.png){#fig-bagging-results-3 width=672 height=523}\n:::\n\nBagging Results\n:::\n\n\n# Boosting\nBoosting is a machine learning ensemble technique that aims to convert weak learners into strong ones. It involves sequentially training a series of weak models and adjusting their weights based on performance to create a powerful ensemble model. The key idea is to combine several weak learners to form a strong learner, which can improve the overall performance and predictive accuracy compared to any individual model in the ensemble. Boosting works by assigning a weight to each training example, which is modified during the training process based on the performance of the models. Examples that are misclassified by the previous models are given higher weights so that subsequent models focus more on these examples. As a result, the final model focuses more on the difficult-to-classify examples, effectively reducing the overall error.\n\n## Python implimentation\n@fig-boosting-results visualizes the results of Boosting Classifier on the Breast Cancer. @fig-boosting-results-1 shows the confusion matrix, @fig-boosting-results-2 shows the RoC Curve, and  @fig-boosting-results-3 shows the Precision Recall Cruve of Boosting Classifier on the Breast Cancer\n\n::: {#fig-boosting-results .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport numpy as  np\nfrom itertools import cycle\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load the Breast Cancer dataset\nX = data.iloc[:,2:-1]\ny = data[['diagnosis']]\ny = label_binarize(y, classes=['M','B'])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n\n# Initialize the base classifiers\ndt = DecisionTreeClassifier()\n\n# Initialize the stacking classifier\nclf = AdaBoostClassifier(estimator=dt, n_estimators=10)\n\n# Fit the model on the training data\nclf.fit(X_train, y_train)\n\n# Evaluate the model on the test data\ny_test_pred = clf.predict(X_test)\ny_test_proba = clf.predict_proba(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_test_pred))\nprint(\"Precision:\", metrics.precision_score(y_test, y_test_pred, average='weighted'))\nprint(\"Recall:\", metrics.recall_score(y_test, y_test_pred, average='weighted'))\nprint(\"sensitivity:\", metrics.recall_score(y_test, y_test_pred, average='weighted'))\nprint(\"f1 score:\", metrics.f1_score(y_test, y_test_pred, average='weighted'))\nprint(metrics.classification_report(y_test, y_test_pred, target_names = ['M', 'B']))\ncm = metrics.confusion_matrix(y_test, y_test_pred)\nplt.figure(figsize=(4, 3), dpi=600)\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nclasses = ['M', 'B']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes)\nplt.yticks(tick_marks, classes)\nthresh = cm.max() / 2.\nfor i, j in np.ndindex(cm.shape):\n    plt.text(j, i, cm[i, j],\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] > thresh else \"black\")\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n# Compute ROC curve and ROC area\nfpr, tpr, _ = metrics.roc_curve(y_test, y_test_proba[:, 1])\nroc_auc = metrics.auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\nplt.plot([0, 1], [0, 1], 'k--', color='grey', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n# Compute precision-recall curve and area\nprecision, recall, _ = metrics.precision_recall_curve(y_test, y_test_proba[:, 1])\npr_auc = metrics.average_precision_score(y_test, y_test_proba[:, 1])\n\n# Plot Precision-Recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label=f'AUC = {pr_auc:.2f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"lower left\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.9473684210526315\nPrecision: 0.9473684210526315\nRecall: 0.9473684210526315\nsensitivity: 0.9473684210526315\nf1 score: 0.9473684210526315\n              precision    recall  f1-score   support\n\n           M       0.93      0.93      0.93        43\n           B       0.96      0.96      0.96        71\n\n    accuracy                           0.95       114\n   macro avg       0.94      0.94      0.94       114\nweighted avg       0.95      0.95      0.95       114\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Confusion Matrix](index_files/figure-html/fig-boosting-results-output-2.png){#fig-boosting-results-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![RoC Curve](index_files/figure-html/fig-boosting-results-output-3.png){#fig-boosting-results-2 width=663 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![Precision Recall Curve](index_files/figure-html/fig-boosting-results-output-4.png){#fig-boosting-results-3 width=672 height=523}\n:::\n\nBoosting Results\n:::\n\n\n# Conclusion\nEnsemble methods have revolutionized the field of classification by significantly improving the accuracy and robustness of predictive models. This blog aims to provide a comprehensive understanding of ensemble techniques and their practical implementation using Python. By highlighting the importance of evaluation metrics visualization, we emphasize the significance of performance analysis in assessing the effectiveness of classification models.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}