{
  "hash": "6269b8cc93021bc4d0781836515a6305",
  "result": {
    "markdown": "---\ntitle: Understanding Clustering\nimage: image.png\nauthor: Swapnil Singh\ndate: '2023-11-05'\ncategories:\n  - clustering\n  - unsupervised learning\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n---\n\nClustering is a fundamental technique in unsupervised learning, used to group similar data points together. In this blog, we will explore three popular clustering algorithms: KMeans, DBSCAN, and Agglomerative Clustering. We'll provide a brief overview of each algorithm, discuss their applications, and provide Python code examples with visualizations to demonstrate how they work.\n\n# Introduction to Clustering Algorithms\n\n## KMeans Clustering\n\nKMeans is an iterative algorithm that partitions data into K clusters. It works by assigning data points to the nearest cluster center and updating the center as the mean of the assigned points. This process continues until convergence.\n\n## DBSCAN Clustering\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm. It groups together points that are closely packed and marks points that lie alone in low-density regions as outliers.\n\n## Agglomerative Clustering\n\nAgglomerative Clustering is a hierarchical clustering technique that starts with each point as a separate cluster and merges the closest clusters iteratively until only one cluster remains.\n\n# Python Code Examples\n## KMeans Clustering\n@fig-kmean visualises the clusters created after kmeans clustering. @fig-kmean-1 displays the correlation heat map to identify the most correlated features. It is observed that Annual Income (k$) and Spending Socre (1-100) are the most correlated features. @fig-kmean-2 shows the elbow curve indicating that 4 clusters would be better to use and @fig-kmean-3 shows the clusters after clustering.\n\n::: {#fig-kmean .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport warnings\nimport pandas as pd\nwarnings.filterwarnings(\"ignore\")\n\ndf = pd.read_csv('Mall_Customers.csv')\nX = df[['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n\n\n\ncorr_matrix = np.corrcoef(X.values, rowvar=False)\nplt.imshow(corr_matrix, cmap='Wistia', interpolation='nearest')\nfor i in range(corr_matrix.shape[0]):\n    for j in range(corr_matrix.shape[1]):\n        plt.text(j, i, f'{corr_matrix[i, j]:.2f}', ha='center', va='center', color='black')\nplt.title('Heatmap for the Data', fontsize=20)\nplt.xticks(np.arange(corr_matrix.shape[0]), labels=['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)'], rotation=90)\nplt.yticks(np.arange(corr_matrix.shape[1]), labels=['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)'])\nplt.colorbar()\nplt.show()\n\nX = X.values\ndist = [] \nfor i in range(1,11): \n  km = KMeans(n_clusters=i, random_state=42).fit(X)\n  dist.append(km.inertia_)\nplt.plot(range(1,11), dist, marker='*')\nplt.xlabel('Number of cluster')\nplt.ylabel('Distortion')\nplt.show()\n\n# Applying KMeans algorithm\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\ncenters = kmeans.cluster_centers_\n\n# Visualizing clusters with different colors and a legend\nplt.scatter(X[y_kmeans == 0, 2], X[y_kmeans == 0, 3], c='yellow', s=50, cmap='viridis', label='Cluster1')\nplt.scatter(X[y_kmeans == 1, 2], X[y_kmeans == 1, 3], c='blue', s=50, cmap='viridis', label='Cluster2')\nplt.scatter(X[y_kmeans == 2, 2], X[y_kmeans == 2, 3], c='green', s=50, cmap='viridis', label='Cluster3')\nplt.scatter(X[y_kmeans == 3, 2], X[y_kmeans == 3, 3], c='violet', s=50, cmap='viridis', label='Cluster4')\nplt.scatter(centers[:, 2], centers[:, 3], c='red', s=200, alpha=0.5, label='Centroids')\nplt.legend()\nplt.show()\n\nprint('Silhouette Score: ',metrics.silhouette_score(X, km.labels_, metric='euclidean'))\n```\n\n::: {.cell-output .cell-output-display}\n![Correlation Heat Map to identify correlated features](index_files/figure-html/fig-kmean-output-1.png){#fig-kmean-1 width=641 height=584}\n:::\n\n::: {.cell-output .cell-output-display}\n![Elbow Curve](index_files/figure-html/fig-kmean-output-2.png){#fig-kmean-2 width=589 height=443}\n:::\n\n::: {.cell-output .cell-output-display}\n![Cluster Visualization](index_files/figure-html/fig-kmean-output-3.png){#fig-kmean-3 width=575 height=411}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nSilhouette Score:  0.3724780422340438\n```\n:::\n\nKMeans Clustering\n:::\n\n\n## DBSCAN Clustering\n@fig-dbscan visualises the clusters created after DBScan\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import DBSCAN\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Generate sample data\nX, _ = make_moons(n_samples=1000, noise=0.05)\n\n# Apply DBSCAN algorithm\ndbscan = DBSCAN(eps=0.1, min_samples=5)\ny_pred = dbscan.fit_predict(X)\n\n# Visualize Clusters\nplt.scatter(X[y_pred == 0, 0], X[y_pred == 0, 1], c='lightblue', marker='o', edgecolor='black', label='Cluster 1')\nplt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], c='lightgreen', marker='s', edgecolor='black', label='Cluster 2')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![DBSCAN Clusters Created](index_files/figure-html/fig-dbscan-output-1.png){#fig-dbscan width=590 height=411}\n:::\n:::\n\n\n## Agglomerative Clustering\n@fig-agglomerative visualises the dendogram created after aglomerative clustering on the mall customer dataset\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.cluster import AgglomerativeClustering\nimport pandas as pd\n\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack(\n        [model.children_, model.distances_, counts]\n    ).astype(float)\n\n    # Plot the corresponding dendrogram\n    dendrogram(linkage_matrix, **kwargs)\n\n\ndf = pd.read_csv('Mall_Customers.csv')\nX = df[['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)']].values\n\n# setting distance_threshold=0 ensures we compute the full tree.\nmodel = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n\nmodel = model.fit(X)\nplt.title(\"Hierarchical Clustering Dendrogram\")\n# plot the top three levels of the dendrogram\nplot_dendrogram(model, truncate_mode=\"level\", p=3)\nplt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Agglomerative Clustering Dendogram](index_files/figure-html/fig-agglomerative-output-1.png){#fig-agglomerative width=583 height=452}\n:::\n:::\n\n\n# Conclusion\n\nClustering is a powerful technique for exploring and understanding complex datasets. Each algorithm has its unique strengths and weaknesses, making them suitable for different types of data and applications. By understanding the nuances of each algorithm, you can apply them effectively to uncover hidden patterns and insights in your data.\n\nIn this blog, we explored KMeans, DBSCAN, and Agglomerative Clustering and provided Python code examples for each. We hope this overview helps you get started with clustering and inspires you to explore more complex applications and datasets.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}